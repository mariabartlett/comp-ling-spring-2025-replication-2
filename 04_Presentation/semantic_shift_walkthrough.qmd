---
title: "Analysis: Semantic shift of Trump"
subtitle: "Replicated from _Embedding Regression: Models for Context-Specific Description and Inference_ (Rodriguez et al., 2023)"
author: "Bridgette Sullivan & Maria Bartlett"
date: "`r Sys.Date()`"
format: html
theme: litera
toc: TRUE
toc-location: left
toc-depth: 7
embed-resources: TRUE
linkcolor: "black"
editor: visual
fontsize: 12pt
css: bootstrap.css
page-layout: full
---

```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE
#| echo: FALSE

# clear global environment
rm(list = ls())

# set seed
set.seed(12345)

# provide instruction for how to install user-written packages
#devtools::install_github("prodriguezsosa/conText")

# load packages
library(tidyverse)
library(conText)
library(quanteda)
library(kableExtra)
library(assertr)

# set processing/display options
options(mc.cores = parallel::detectCores())
options(dplyr.summarise.inform = FALSE)
options(warn = -1)

# set relative paths
root       <- file.path(getwd())
author_rep <- file.path(root,"02_DataverseMaterials","dataverse_files")

# define kable styling options
kable_style <- function(df) {
  
  df %>%
  kbl(full_width = T, 
      align = "l",
      booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header","scale_down","HOLD_position")) 
}

```

**Research question:** Was the word "Trump" used differently post-2016 U.S. election relative to the word "Clinton"?

### Step 1: Import 3 required inputs

  1. Corpus (New York Times articles)
  
```{r}
#| warning: false
#| output: asis
#| code-fold: true

# --------------------------------
# load data
# --------------------------------

# nyt corpus
corpus_nyt <- readRDS(file.path(author_rep,"data","nyt_data.rds")) %>%
  # data contains 3 million + articles 
  verify(nrow(.) == 3032517 & ncol(.) == 6) %>%
  verify(names(.) %in% c("snippet","lead_paragraph","web_url","date","year","month")) %>%
  verify(min(year) == 1990 & max(year) == 2020) 

# impute lead paragraph with snippet when lead paragraph is missing
corpus_nyt$lead_paragraph[is.na(corpus_nyt$lead_paragraph)] <- corpus_nyt$snippet[is.na(corpus_nyt$lead_paragraph)] 

```

  2. GloVe pre-trained embeddings (400,000 x 300)
  
```{r}
#| warning: false
#| output: asis
#| code-fold: true

# pre-trained GloVe embeddings 
pre_trained <- readRDS(file.path(author_rep,"data","stanford-glove","glove.rds"))

# confirm GloVe is comprised of 300-dimension embeddings for 400,000 tokens
stopifnot(nrow(pre_trained) == 400000 & ncol(pre_trained) == 300)

# look at head of GloVe embeddings
head(as.data.frame(pre_trained)) %>% kable_style()
 
```
  
  3. Pre-trained transformation matrix (300 x 300)

```{r}
#| warning: false
#| output: asis
#| code-fold: true

# pre-trained transformation matrix (A_hat)
transform_matrix <- readRDS(file.path(author_rep,"data","stanford-glove","khodakA.rds"))

# confirm transformation matrix is 300 x 300
stopifnot(nrow(transform_matrix) == 300 & ncol(transform_matrix) == 300)

# look at head of transformation matrix
head(as.data.frame(transform_matrix)) %>% kable_style()

```

### Step 2: Subset corpus to articles of interest

Keep only articles from the NYT corpus that contain the strings 'Trump' or 'Clinton' (respecting case) between [2011,2014] and [2017,2020].

```{r}
#| warning: false
#| output: asis
#| code-fold: true

# keep only documents where the target words appear: Trump
trump_corpus <- corpus_nyt[grep('Trump', corpus_nyt$lead_paragraph, fixed = TRUE, ignore.case = FALSE), 
                           c('lead_paragraph', 'year')] %>% 
  # 24,815 articles where 'Trump' appears in lead paragraph
  verify(nrow(.) == 24815) %>%
  distinct(lead_paragraph, .keep_all = TRUE) %>% 
  # 17,802 articles once duplicates are dropped (if duplicates, first year in which article appeared is retained)
  verify(nrow(.) == 17802) %>%
  filter(year %in% c(2011:2014,2017:2020)) %>% 
  # 11,675 articles once subset to period of interest
  verify(nrow(.) == 11675) %>%
  rename(text = lead_paragraph) %>% 
  mutate(target = 'trump', 
         year   = as.integer(year)) # uppercase matters here

# keep only documents where the target words appear: Clinton
clinton_corpus <- corpus_nyt[grep('Clinton', corpus_nyt$lead_paragraph, fixed = TRUE, ignore.case = FALSE), 
                             c('lead_paragraph', 'year')] %>% 
  # 45,463 articles where 'Clinton' appears in lead paragraph
  verify(nrow(.) == 45463) %>% 
  distinct(lead_paragraph, .keep_all = TRUE) %>% 
  # 44,591 articles once duplicates are dropped (if duplicates, first year in which article appeared is retained)
  verify(nrow(.) == 44591) %>%
  filter(year %in% c(2011:2014,2017:2020)) %>% 
  # 1,622 articles once subset to period of interest
  verify(nrow(.) == 1622) %>%
  rename(text = lead_paragraph) %>% 
  mutate(target = 'clinton', 
         year   = as.integer(year)) 

# examine distribution of year in Clinton articles
count(clinton_corpus, year) %>% kable_style()

# bind the two target word corpuses 
sub_corpus <- rbind(trump_corpus, clinton_corpus)

# examine distribution of articles in each corpus
count(sub_corpus,target) %>% kable_style()

```

**What does an example text look like at this point?**

```{r}
#| warning: false
#| output: asis
#| echo: true
#| eval: true

sub_corpus$text[1]

```

### Step 3: Preprocess text

```{r}
#| warning: false
#| output: asis
#| code-fold: true

# basic preprocessing of text
sub_corpus$text <- sub_corpus$text %>%
  gsub('Trump', 'toi', .) %>% # replace mentions of Trump with TOI (target of interest)
  gsub('Clinton', 'toi', .) %>% # replace mentions of Clinton with TOI (target of interest)
  gsub('LEAD:', '', .) %>% # remove header banner
  gsub("[^[:alpha:]]", " ", .) %>% # remove all non-alpha characters
  str_replace_all("\\b\\w{1,2}\\b", "") %>% # remove 1-2 letter words
  str_replace_all("^ +| +$|( ) +", "\\1") %>% # remove excess white space
  tolower() # lowercase

```

**What does the text from above look like at this point?**

```{r}
#| warning: false
#| output: asis
#| echo: true
#| eval: true

print(sub_corpus$text[1], max_nchar = 1000)

```

### Step 4: Identify covariates

Covariates of interest:

- $Trump$: Whether article leading text contains string 'Trump' (vs. 'Clinton')

- $Post\_election$: Whether article was published during 2017-2020 (vs. 2011-2014)

- $Trump X Post\_election$: Interaction term

```{r}
#| warning: false
#| output: asis
#| code-fold: true

# add dummy variables distinguishing pre-/post-election years and trump/clinton mentions
sub_corpus <- sub_corpus %>% 
  mutate(post_election = if_else(year>2014, 1L, 0L),
         trump         = if_else(target == 'trump', 1L, 0L)) %>%
  mutate(interaction = trump*post_election)

# transform into quanteda corpus
sub_corpus <- corpus(sub_corpus$text, docvars = sub_corpus[,c("year", "target", "post_election", "trump", "interaction")])
toks       <- tokens(sub_corpus)

```

### Step 5: `conText` regression

$Focal\_word\_embedding = \beta_0 + \beta_1Trump + \beta_2Post\_election + \beta_3Trump X Post\_election + \epsilon$

**How is this outcome variable getting calculated in the regression?**

Context window: _and other major infrastructure projects according_ **toi** _administration plan that would weaken the_ 

```{r}
#| warning: false
#| output: asis
#| echo: true
#| eval: true

# average 12 embeddings in 6-word window
embed <- (pre_trained["and",] + pre_trained["other",] + pre_trained["major",] + pre_trained["infrastructure",] + 
          pre_trained["projects",] + pre_trained["according",] + pre_trained["administration",] + 
          pre_trained["plan",] + pre_trained["that",] + pre_trained["would",] + pre_trained["weaken",] + pre_trained["the",]) / 12 

as.data.frame(as.matrix(embed)) %>% head() %>% kable_style() # 300 x 1

# multiply by transformation matrix
transform <- transform_matrix %*% as.matrix(embed)

as.data.frame(as.matrix(transform)) %>% head() %>% kable_style() # this is the embedding outcome!

```

#### conText regression

```{r}
#| warning: false
#| output: false
#| code-fold: false

# run regression
set.seed(2022L)

model1 <- conText(formula              = toi ~ trump + post_election + interaction, 
                  data                 = toks,              # tokenized articles containing 'Trump' or 'Clinton'
                  pre_trained          = pre_trained,       # GloVe pre-trained embeddings matrix
                  transform            = TRUE,              # apply ALC transformation matrix (A_hat)
                  transform_matrix     = transform_matrix,  # ALC transformation matrix (300x300)
                  bootstrap            = TRUE,              # bootstrap SEs
                  jackknife            = FALSE,  
                  num_bootstraps       = 1000,              # number of bootstraps to use 
                  confidence_level     = 0.95,              # 95% confidence level
                  stratify             = TRUE,              # stratify by covariates
                  permute              = TRUE,              # use permutation testing for hypothesis testing
                  num_permutations     = 100,               # number of permutations
                  window               = 6,                 # number of tokens to look at on either side of keyword
                  valuetype            = 'fixed',           # exact matching
                  hard_cut             = FALSE,             # allows for target word to be first word
                  verbose              = TRUE)              # report documents omitted because no features with pretrained embeddings

```

#### Examine returned coefficients

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# normed betas
model1@normed_coefficients %>%
  kable_style()

```

#### Examine coefficient plot

```{r}
#| warning: false
#| output: asis
#| code-fold: true

# coefficient plot
plot_tibble <- model1@normed_coefficients %>% 
  mutate(coefficient = c("Trump", "Post_Election", "Trump x \n Post_Election")) %>% 
  mutate(coefficient = factor(coefficient, levels = coefficient))

ggplot(plot_tibble, aes(x = coefficient, y = normed.estimate)) +
  geom_pointrange(aes(ymin = lower.ci, ymax = upper.ci), size = 1) +
  labs(y = expression(paste('Norm of ', hat(beta),'s'))) +
  geom_text(aes(label=c('***', '***', '***')), position=position_dodge(width=0.9), hjust=0.5, vjust = c(0, 0, 0), size = 8) +
  coord_flip() +
  ylim(0,0.6) +
  theme(axis.text.x = element_text(size=18, vjust = 0.5, margin = margin(t = 15, r = 0, b = 15, l = 0)),
        axis.text.y = element_text(size=18),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size=20),
        plot.margin=unit(c(1,1,0,0),"cm"))

```
