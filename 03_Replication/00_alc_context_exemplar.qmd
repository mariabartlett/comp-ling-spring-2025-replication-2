---
title: "Replication Code: A Replication of _Embedding Regression: Models for Context-Specific Description and Inference_ (Rodriguez et al., 2023)"
subtitle: "PPOL 6801: Text as Data: Computational Linguistics"
author: "Bridgette Sullivan & Maria Bartlett"
date: "`r Sys.Date()`"
format: html
theme: litera
toc: TRUE
toc-location: left
toc-depth: 7
embed-resources: TRUE
linkcolor: "black"
editor: visual
fontsize: 12pt
css: bootstrap.css
page-layout: full
---

### Set-up

```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE

# clear global environment
rm(list = ls())

# set seed
set.seed(12345)

# provide instruction for how to install user-written packages
#devtools::install_github("prodriguezsosa/conText")

# load packages
library(tidyverse)
library(conText)
library(quanteda)
library(cluster)
library(assertr)

# set processing/display options
options(mc.cores = parallel::detectCores())
options(dplyr.summarise.inform = FALSE)
options(warn = -1)

# set relative paths
root       <- file.path(getwd() %>% dirname())
author_rep <- file.path(root,"02_DataverseMaterials","dataverse_files")

```

#### Load data

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# --------------------------------
# load data
# --------------------------------

# nyt corpus
corpus_nyt <- readRDS(file.path(author_rep,"data","fg03_corpus.rds"))

stopifnot(length(corpus_nyt) == 15037)  # contains 15,037 lead paragraphs from articles

# pre-trained GloVe embeddings 
pre_trained <- readRDS(file.path(author_rep,"data","stanford-glove","glove.rds"))

# confirm GloVe is comprised of 300-dimension embeddings for 400,000 tokens
stopifnot(nrow(pre_trained) == 400000 & ncol(pre_trained) == 300)
 
# pre-trained transformation matrix (A_hat)
transform_matrix <- readRDS(file.path(author_rep,"data","stanford-glove","khodakA.rds"))

# confirm transformation matrix is 300 x 300
stopifnot(nrow(transform_matrix) == 300 & ncol(transform_matrix) == 300)

```

#### ALC embedding

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# --------------------------------
# ALC embed
# --------------------------------

# tokenize corpus
toks <- tokens(corpus_nyt, 
               remove_punct = T, 
               remove_symbols = T, 
               remove_numbers = T, 
               remove_separators = T)

# examine structure of tokenized corpus
toks

# build a tokenized corpus of contexts sorrounding the target
trump_toks <- tokens_context(x = toks, 
                             pattern = c('trump', 'Trump'), 
                             valuetype = "fixed", 
                             window = 6L, 
                             case_insensitive = FALSE, 
                             hard_cut = TRUE,
                             verbose = FALSE)

# there is one instance where both Trump and trump appear, hence target!=pattern
trump_toks <- tokens_subset(trump_toks, target == pattern) 

# sample 400 instances of each sense (there are 403 instances of trump)
set.seed(2022L)
trump_toks_sample <- tokens_sample(x = trump_toks, size = 400, replace = FALSE, by = docvars(trump_toks,'target'))

# build a document-feature-matrix
trump_dfm <- dfm(trump_toks_sample, tolower = TRUE)

# embed each instance using ALC
trump_dem <- dem(x = trump_dfm, pre_trained = pre_trained, transform = TRUE, transform_matrix = transform_matrix, verbose = FALSE)

# force individual data points into two clusters
trump_clusters <- kmeans(trump_dem, 2, nstart = 100)

# find principal components using pca
trump_pca <- prcomp(trump_dem, scale = TRUE)

# first two pcs
ind_coord <- as_tibble(trump_pca$x[,1:2])

# tibble for plotting
plot_tibble <- tibble(doc_id = 1:length(trump_toks_sample), ind_coord, text = unname(sapply(trump_toks_sample, function(i) paste0(i, collapse = " "))), target = docvars(trump_toks_sample, 'target'), cluster = unname(trump_clusters$cluster))

# identify majority label in each cluster and misclassification
trump_cluster <- which(as.vector(table(plot_tibble$target, plot_tibble$cluster)['trump',]) == max(table(plot_tibble$target, plot_tibble$cluster)['trump',]))
Trump_cluster <- which(as.vector(table(plot_tibble$target, plot_tibble$cluster)['Trump',]) == max(table(plot_tibble$target, plot_tibble$cluster)['Trump',]))
plot_tibble <- plot_tibble %>% mutate(cluster = if_else((target == 'trump' & cluster!=trump_cluster) | (target == 'Trump' & cluster!=Trump_cluster), 3L, cluster))
plot_tibble <- plot_tibble %>% mutate(target = if_else(cluster == 3L, 'misclassified', target))
plot_tibble <- plot_tibble %>% mutate(target = factor(target, levels = c('trump', 'misclassified', 'Trump')))

```

Figure 2

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# --------------------------------
# plot
# --------------------------------
ggplot(plot_tibble, aes(x = PC1, y = PC2, color = target, shape = target)) +  
  geom_point(size = 4) +
  geom_hline(yintercept = 0, linetype="dashed", color = "black", size = 0.5) + 
  geom_vline(xintercept = 0, linetype="dashed", color = "black", size = 0.5) +
  scale_colour_manual(labels = c('Trump', 'misclassified', 'trump'),
                      values = c("red", "grey20", "blue")) +   
  scale_shape_manual(labels = c('Trump', 'misclassified', 'trump'),
                     values = c(19, 4, 17)) +
  xlab('PC1') + ylab('PC2') +
  #xlim(-12, 12) + ylim(-10,10) +
  theme(panel.background = element_blank(),
        axis.text.x = element_text(size=18),
        axis.text.y = element_text(size=18),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 15)),
        axis.title.x = element_text(size=20, margin = margin(t = 15, r = 0, b = 15, l = 0)),
        legend.text=element_text(size=18),
        legend.title=element_blank(),
        legend.key=element_blank(),
        legend.position = "top",
        legend.spacing.x = unit(0.25, 'cm'))

```

#### Semantic shift of Trump

Figure 4

```{r}
#| warning: false
#| output: asis
#| code-fold: false

# targets
targets <- c('Trump', 'Clinton')

# keep only documents where the target words appear
trump_corpus <- corpus_nyt[grep('Trump', corpus_nyt$lead_paragraph, fixed = TRUE, ignore.case = FALSE), c('lead_paragraph', 'year')] %>% distinct(lead_paragraph, .keep_all = TRUE) %>% filter(year %in% c(2011:2014,2017:2020)) %>% rename(text = lead_paragraph) %>% mutate(target = 'trump', year = as.integer(year)) # uppercase matters here
clinton_corpus <- corpus_nyt[grep('Clinton', corpus_nyt$lead_paragraph, fixed = TRUE, ignore.case = FALSE), c('lead_paragraph', 'year')] %>% distinct(lead_paragraph, .keep_all = TRUE) %>% filter(year %in% c(2011:2014,2017:2020)) %>% rename(text = lead_paragraph) %>% mutate(target = 'clinton', year = as.integer(year)) # uppercase matters here
sub_corpus <- rbind(trump_corpus, clinton_corpus)

# basic preprocessing of text
sub_corpus$text <- sub_corpus$text %>%
  gsub('Trump', 'toi', .) %>% # replace mentions of Trump with TOI (target of interest)
  gsub('Clinton', 'toi', .) %>% # replace mentions of Clinton with TOI (target of interest)
  gsub('LEAD:', '', .) %>% # remove header banner
  gsub("[^[:alpha:]]", " ", .) %>% # remove all non-alpha characters
  str_replace_all("\\b\\w{1,2}\\b", "") %>% # remove 1-2 letter words
  str_replace_all("^ +| +$|( ) +", "\\1") %>% # remove excess white space
  tolower() # lowercase

#---------------------------------
# conText regression
#---------------------------------

# add dummy variables distinguishing pre-/post-election years and trump/clinton mentions
sub_corpus <- sub_corpus %>% mutate(post_election = if_else(year>2014, 1L, 0L))
sub_corpus <- sub_corpus %>% mutate(trump = if_else(target == 'trump', 1L, 0L))
sub_corpus <- sub_corpus %>% mutate(interaction = trump*post_election)

# transform into quanteda corpus
sub_corpus <- corpus(sub_corpus$text, docvars = sub_corpus[,c("year", "target", "post_election", "trump", "interaction")])
toks <- tokens(sub_corpus)

# run regression
set.seed(2022L)
model1 <- conText(formula =  toi ~ trump + post_election + interaction, data = toks, pre_trained = pre_trained, transform = TRUE, transform_matrix = transform_matrix, bootstrap = TRUE, num_bootstraps = 1000, confidence_level = 0.95, stratify = TRUE, permute = TRUE, num_permutations = 100, window = 6, valuetype = 'fixed', case_insensitive = TRUE, hard_cut = FALSE, verbose = TRUE)

# save results
saveRDS(model1, 'data/fg04_output.rds')

#---------------------------------
# visualize
#---------------------------------

# read results
model1 <- readRDS('data/fg04_output.rds')
model1@normed_coefficients

# coefficient plot
plot_tibble <- model1@normed_coefficients %>% mutate(coefficient = c("Trump", "Post_Election", "Trump x \n Post_Election")) %>% mutate(coefficient = factor(coefficient, levels = coefficient))
fg4 <- ggplot(plot_tibble, aes(x = coefficient, y = normed.estimate)) +
  geom_pointrange(aes(ymin = lower.ci, ymax = upper.ci), size = 1) +
  labs(y = expression(paste('Norm of ', hat(beta),'s'))) +
  geom_text(aes(label=c('***', '***', '***')), position=position_dodge(width=0.9), hjust=0.5, vjust = c(0, 0, 0), size = 8) +
  coord_flip() +
  ylim(0,0.6) +
  theme(axis.text.x = element_text(size=18, vjust = 0.5, margin = margin(t = 15, r = 0, b = 15, l = 0)),
        axis.text.y = element_text(size=18),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size=20),
        plot.margin=unit(c(1,1,0,0),"cm"))

ggsave(filename = "fg04.pdf", plot = fg4, height = 10, width = 12, path = './figures/', dpi = 1000)

```
