---
title: "Replication Code: A Replication of _Embedding Regression: Models for Context-Specific Description and Inference_ (Rodriguez et al., 2023)"
subtitle: "PPOL 6801: Text as Data: Computational Linguistics"
author: "Bridgette Sullivan & Maria Bartlett"
date: "`r Sys.Date()`"
format: html
theme: litera
toc: TRUE
toc-location: left
toc-depth: 7
embed-resources: TRUE
linkcolor: "black"
editor: visual
fontsize: 12pt
css: bootstrap.css
page-layout: full
---

### Set-up

```{r}
#| message: FALSE
#| warning: FALSE
#| code-fold: TRUE

# clear global environment
rm(list = ls())

# set seed
set.seed(12345)

# provide instruction for how to install user-written packages
#devtools::install_github("prodriguezsosa/conText")

# load packages
library(tidyverse)
library(conText)
library(quanteda)
library(cluster)
library(assertr)

# set processing/display options
options(mc.cores = parallel::detectCores())
options(dplyr.summarise.inform = FALSE)
options(warn = -1)

# set relative paths
root       <- file.path(getwd() %>% dirname())
author_rep <- file.path(root,"02_DataverseMaterials","dataverse_files")

```

### Case 2: Extension

#### Validation 1: Counts

```{r}

# load corpus
empire_corpus <- readRDS(file.path(author_rep, "data/fg07_corpus.rds"))

# pre-trained embeddings & transformation matrix (local)
#pre_trained <- readRDS(file.path(author_rep, "word_vectors_6_300_5000.rds"))  # cr + ps pre-trained embeddings
#transform_matrix <- readRDS(file.path(author_rep, "A_local.rds"))  # transformation matrix for cr + ps embeddings

# convert empire corpus to df for plotting
empire_text_df = convert(empire_corpus, to = 'data.frame')

# create bar chart for counts by country group
p = empire_text_df %>%
  ggplot(aes(x = period, fill = group)) + 
  geom_bar(position = 'dodge') + 
  theme(axis.text.x=element_text(angle=90, hjust=1)) +
  labs(title = "Count of 'Empire' in US v. UK", fill = 'Group') +
  ylab("Count of 'Empire'") +
  xlab("Time Period") 


p
```

Validation 2: Cosine Similarity of Embeddings

```{r}
#| warning: false
#| output: asis
#| code-fold: false

library(text2vec)
library(dplyr)
library(stringr)
library(hunspell)
library(data.table)

# ================================
# choice parameters
# ================================
WINDOW_SIZE <- 6
DIM <- 300
ITERS <- 20
TERM_MAX <- 5000

# ================================
# define paths
# ================================
# set working directory to the location of the master "EmbeddingRegressionReplication" folder
#setwd("")

# ================================
# load data
# ================================

cr_data_compressed <- fread(file.path(author_rep, "cr_data_compressed.csv")) # Faster loading than readRDS
#head(cr_data_compressed)
ps_data_compressed <- fread(file.path(author_rep, "ps_data_compressed.csv"))

# Separate into pre- and post-1949
cr_pre <- cr_data_compressed %>% filter(period_start < 1949 & period_start > 1935) %>% pull(speech)
cr_post <- cr_data_compressed %>% filter(period_start >= 1949 & period_start < 2009) %>% pull(speech)

ps_pre <- ps_data_compressed %>% filter(year < 1949 & year > 1935) %>% pull(text)
ps_post <- ps_data_compressed %>% filter(year >= 1949 & year < 2009) %>% pull(text)


# ================================
# create function to compute glove word embeddings for a corpus
# ================================

compute_embeddings <- function(text_data, window = 6, dim = 300, iters = 20) {
  text_data <- text_data %>%
    iconv(from = "latin1", to = "UTF-8") %>%
    gsub("[^[:alpha:]]", " ", .) %>%
    str_replace_all("\\b\\w{1,2}\\b", "") %>%
    str_replace_all("^ +| +$|( ) +", "\\1") %>%
    tolower() %>%
    unique()
  
  tokens <- space_tokenizer(text_data)
  it <- itoken_parallel(tokens, n_chunks = 4)
  vocab <- create_vocabulary(it) %>% prune_vocabulary(term_count_min = 10)
  vectorizer <- vocab_vectorizer(vocab)
  tcm <- create_tcm(it, vectorizer, skip_grams_window = window)
  
  glove <- GlobalVectors$new(rank = dim, x_max = 100, learning_rate = 0.05)
  wv_main <- glove$fit_transform(tcm, n_iter = iters)
  wv_context <- glove$components
  word_vectors <- wv_main + t(wv_context)
  return(word_vectors)
}

# ================================
# compute embeddings for each time period
# ================================

# Commented out to reduce run time, but included to show methodology.

#wv_cr_pre <- compute_embeddings(cr_pre)
#wv_cr_post <- compute_embeddings(cr_post)
#wv_ps_pre <- compute_embeddings(ps_pre)
#wv_ps_post <- compute_embeddings(ps_post)

# Save only the empire row to save space

#fwrite(data.frame(wv_cr_pre['empire', ]), file.path(author_rep, "wv_cr_pre_empire.csv")) 
#fwrite(data.frame(wv_cr_post['empire', ]), file.path(author_rep, "wv_cr_post_empire.csv")) 
#fwrite(data.frame(wv_ps_pre['empire', ]), file.path(author_rep, "wv_ps_pre_empire.csv")) 
#fwrite(data.frame(wv_ps_post['empire', ]), file.path(author_rep, "wv_ps_post_empire.csv")) 
 
wv_cr_pre_empire <- fread(file.path(author_rep, "wv_cr_pre_empire.csv"))
wv_cr_post_empire <- fread(file.path(author_rep, "wv_cr_post_empire.csv"))
wv_ps_pre_empire <- fread(file.path(author_rep, "wv_ps_pre_empire.csv"))
wv_ps_post_empire <- fread(file.path(author_rep, "wv_ps_post_empire.csv"))


# ================================
# calculate cosine similarity across groups for each time period
# ================================

library(text2vec)
library(knitr)

cos_sim <- function(a, b) sum(a * b) / (sqrt(sum(a^2)) * sqrt(sum(b^2)))

empire_similarity <- tibble(
  period = c("pre 1949", "post 1949"),
  cosine_cr_ps = c(
    cos_sim(wv_cr_pre_empire, wv_ps_pre_empire),
    cos_sim(wv_cr_post_empire,wv_ps_post_empire)
  )
)

# print comparison
kable(empire_similarity, caption = "Cosine Similarity of 'Empire' Between Congressional Records and Parliamentary Speeches")

```
